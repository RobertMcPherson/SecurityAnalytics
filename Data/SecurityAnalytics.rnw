\documentclass[compress,8pt]{beamer}
\usepackage[latin1]{inputenc}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{url} %needed for function to handle underscore characters
\usepackage[section]{placeins}
\usepackage{Sweave}
\usepackage{upquote}
\SweaveOpts{keep.source=TRUE}
\urlstyle{sf} %style consistent with URL package - can also use rm style
\usetheme{Singapore}%define the beamer theme
\setbeamertemplate{navigation symbols}{}%remove navigation symbols
\setbeamertemplate{section in head/foot shaded}[default][20]
\setbeamertemplate{headline}{}%remove the navigation header
%\setbeamertemplate{subsection in head/foot shaded}[default][20]
%\logo{\includegraphics[width=0.15\textwidth]{nw_logo.jpg}} %change here. I used an eps file logo, you can use jpg or other format
\AtBeginDocument{ 
\DefineVerbatimEnvironment{Sinput}{Verbatim} {xleftmargin=2em,fontsize= 
\footnotesize} 
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em,fontsize= 
\footnotesize} 
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em,fontsize= 
\footnotesize} 
} 

\title{\bfseries{Utilizing the R Statistical Language for Analyzing Server Logs}}
\author{Ryan Talabis and Robert McPherson}
\date{January 26, 2013}
%\institute[Product and Pricing Risk Management]{Product and Pricing Risk Management \\ Nationwide Insurance }

\AtBeginSection[]
{
  \begin{frame}
    \begin{center}
        \Large{\insertsection}
    \end{center}
  \end{frame}
}

\begin{document}

\begin{frame}
  \maketitle
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Processing the Data}

<<name="setup",echo=FALSE,quiet=TRUE,results=hide>>=
#Remove any objects from the environment before starting
rm(list=ls())

#Load libraries
library(tm)
library(stringr)
library(sqldf)
library(data.table)
library(outliers)
library(car) #For Bonferoni outlier test
library(xtable)
@ 

\begin{frame}[containsverbatim] 
  \frametitle{Read in the Data}
  \begin{itemize}
  \item Data can be read from text files of server logs.
  \item An R function called scan is useful for this.
  \item Corpus function is used to convert data into a corpus, consisting of documents and words.
  \item Each line in the log can be considered a document, so that comparisons can be made across time.  
  \end{itemize}
<<name="readData",echo=TRUE,quiet=TRUE,results=tex>>=
#Read server log - text file
secureData <- scan("secure.txt", character(0), sep = "\n") # separate each line

#Save as a word corpus
data <- Corpus(VectorSource(secureData[1:1000],))

inspect(data[1:5]) #Inspect the first five entries in the data
@ 
\end{frame}

\begin{frame}[containsverbatim] 
  \frametitle{Function to Trim White Space}  
  \begin{itemize}
  \item Create a function called, ``trim'', as a convenient way to remove white space in the data transformation process.
  \item Utilizes the R function, gsub, and regular expressions.
  \end{itemize}  
<<name="functionStripWhiteSpace",echo=TRUE,quiet=TRUE,results=tex>>=

#Function to strip white space from strings
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
@
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Using Regular Expressions to Parse the Time Stamp}  
\begin{itemize}
\item Regular expressions provide a common sytax for parsing characters and text.
\item Regular expressions are strings which locate specific patterns in text.
\item These regular expressions extract text patterns which identify time, day, and month from the date stamp in the log file.
\end{itemize}  
<<name="timeStampRegularExpressions",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Regular expressions for parsing date information
regexpTime = "[0-9][0-9]:[0-9][0-9]:[0-9][0-9]"
regexpDay = "\\s[0-9][0-9]\\s"
regexpMonth = "^[A-Z][a-z][a-z]\\s"

time = trim(str_extract(secureData[1:1000],regexpTime))
day = trim(str_extract(secureData[1:1000],regexpDay))
month = trim(str_extract(secureData[1:1000],regexpMonth))
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Results from Parsing the Time Stamp}  
\begin{itemize}
\item Following is a sample from the results extracted from the time stamp.
\item The cbind function means, column bind, and binds arrays into columns.  
\item The data.frame function puts the columnar data into a single table.
\end{itemize}  
<<name="timeStampRegularExpressions",echo=TRUE,quiet=TRUE,results=verbatim>>=  
#Make a data frame consisting of month, day, and time columns
timeStampCols = data.frame(cbind(month, day, time))
timeStampCols[1:5,]
@ 
\end{frame}

\begin{frame}
\frametitle{Transforming the Text Corpus with the tm\_map Function}    
\begin{itemize}
\item The tm\_map function is from the tm text mining package.
\item It maps a number of text mining transformations to the data.
\item Transformations include stripping whitespace, changing to lower case, removing common stopwords, removing word suffixes - called stemming, removing punctuation, and removing numbers, among others.
\end{itemize}  
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Examples of the tm\_map Function}    
<<name="transformations",echo=TRUE,quiet=TRUE,results=verbatim>>=  
data2 = tm_map(data, stripWhitespace)
data2 = tm_map(data2, tolower)
stopWords = c(stopwords("english"),"uiftp","sshd")
data2 = tm_map(data2, removeWords, stopWords)
data2 = tm_map(data2, stemDocument)
data2 = tm_map(data2, removePunctuation)
data2 = tm_map(data2, removeNumbers)

inspect(data2[1:5])
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Making a Document Term Matrix with the DocumentTermMatrix Function}  \begin{itemize}
\item A document term matrix shows the frequency count of each word.
\item Frequencies are shown for each document.
\item Documents are in rows.
\item Terms, or words, are in columns.
\end{itemize}  
<<name="documentTermMatrix",echo=TRUE,quiet=TRUE,results=verbatim>>=  
#Make a word frequency matrix, with documents as rows, and terms as columns
dtm = DocumentTermMatrix(data2)
inspect(dtm[1:5,1:5])
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Removing Sparse Terms}    
\begin{itemize}
\item It can be useful to remove infrequently occuring words.
\item Sometimes the most significant terms are the most fequently occuring ones.
\item Use the removeSparseTerms function. 
\end{itemize}  
<<name="removeSparseTerms",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Remove and inspect sparse terms, with at least 80% sparse occurence
dtm = removeSparseTerms(dtm, 0.85)
inspect(dtm[1:5,1:5])
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Transposing the Term Matrix}    
\begin{itemize}
\item Can opt to have the terms in rows, and the documents in columns. 
\item Use the TermDocumentMatrix for this.
\item Same as the DocumentTermMatrix, but transposed.
\end{itemize}  
<<name="termDocumentMatrix",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Make a word frequency matrix, with terms as rows, and documents as columns
dtm2 = TermDocumentMatrix(data2)
inspect(dtm2[1:5,1:5])
@ 
\end{frame}

\section{Date and Time Stamp Handling}

\begin{frame}[containsverbatim] 
\frametitle{Append Time Element Columns to Text Frequency Columns}    
\begin{itemize}
\item Convert the word corpus to text, so that it can be appended.
\item The example below includes a version with all time stamp elements, and another with only the month.
\end{itemize}  
<<name="appendTimeStamp",echo=TRUE,quiet=TRUE,results=hide>>=  

#Add month, day, and time columns to the document term matrix
#First, turn the dtm into a plain text object
dtmText = inspect(dtm)

#This data frame version has the full month, day, and time stamp 
dtmStamp = data.frame(cbind(timeStampCols, dtmText))

#This data frame version only has the month appended, not day or time
dtmMonth = data.frame(cbind(month, dtmText))
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Utilize SQL as One Means of Grouping Data}    
\begin{itemize}
\item Utilize the sqldf function.
\item This function allows for writing SQL queries against an R data frame.
\end{itemize}  
<<name="sqldfQueries",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Sum up the frequencies each of the top terms for each month
topTermsByMonth = sqldf("select month
,count(*) as rowCount
,sum(user) as user
,sum(invalid) as invalid
,sum(richard) as richard
,sum(port) as port
,sum(password) as password
,sum(ssh) as ssh
,sum(uid) as uid
,sum(session) as session
from dtmMonth group by month")

topTermsByMonth
@ 
\end{frame}

\section{Text Analysis}

\begin{frame}[containsverbatim] 
\frametitle{Finding High Frequency Terms}    
\begin{itemize}
\item Frequently occuring words can be identified with the findFreqTerms function
\item Can select words that occur within a low and high frequency range
\item findFreqTerms(x, lowfreq = 0, highfreq = Inf) 
\item Can also specify only a single, lower bound, with upper bound defaulting to infinity  
\end{itemize}  
<<name="findFrequentTerms",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Find the top 20 most frequent terms
findFreqTerms(dtm, 20)
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Find Associated Terms}    
\begin{itemize}
\item Can find terms in the text that are associated with a given term.
\item Use the findAssocs function
\item Can specify the minimum correlation.
\end{itemize}  
<<name="findAssociatedTerms",echo=TRUE,quiet=TRUE,results=verbatim>>=  

#Find associated terms with the term, "root", and a correlation of at least 0.4
findAssocs(dtm, "session", 0.4)
findAssocs(dtm, "user", 0.4)
findAssocs(dtm, "port", 0.4)
@ 
\end{frame}

\begin{frame}[containsverbatim] 
\frametitle{Create a Dictionary}    
\begin{itemize}
\item Create a dictionary specifying a subset of terms to work with
\item Use the Dictionary function.
\end{itemize}  
<<name="dictionary",echo=TRUE,quiet=TRUE,results=verbatim>>=  

##Other functions related to selecting specific terms

#Create a dictionary: a subset of terms
d = Dictionary(c("root", "richard", "invalid", "session"))

#Use dictionary to make a matrix with only the dictionary terms
dtm_dictionary = TermDocumentMatrix(data2, list(dictionary = d))
show(dtm_dictionary)
@ 
\end{frame}

\section{Outlier Detection}

\begin{frame}
\frametitle{How to Read Outlier Exhibits}
\begin{itemize}
\item Each slide has a table containing outlier(s), and a scatterplot graph
\item This example code produces one slide for every outlier
\item If the occurence frequency of a term happened to be perfectly correlated to the number of log entries for each month, points would line up diagonally on the regression line.
\item Outliers are clearly visible, shown as points that are far
  removed from the regression line.
\item Terms shown above the line are those that had more occurences than expected, given the number of log entries for that month; terms below had less.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Outlier Detection Method}
\begin{itemize}
\item Bonferoni outlier detection method was used to identify outliers.
\item The Bonferroni method is less sensitive to false positives than
  outlier detection based on percentage increases, standard
  deviations (i.e., z-score), or other methods.
  \begin{itemize}
    \item Identifying ouliers based on percentage increase thresholds produces
      far more false positives for small denominator values, and too few
      for large denominators.
    \item The Standard deviation method reduces false positives somewhat,
      but not enough.
    \item The Bonferroni method generally tends to be more balanced across all value ranges.
  \end{itemize}
\item The Bonferroni outlier table at the top of each slide contains statistically significant outlier(s).
\item The Bonferroni method detects outliers based on t-test of regression
  coefficients, at the 99.9\% confidence level.
\item An additional filter was added so that only those states that
  changed by at least plus or minus 10\% can be considered outliers.
\item Without outlier detection, thousands of slides would be
  required to show all the combinations of peril type, business line,
  and financial division, for each and every data attribute.
\end{itemize}
\end{frame}

<<name="outlierDetection",echo=FALSE,quiet=TRUE,results=tex>>=  
sigCutoff = 0.05 #Set significance threshold for p-value
temp = 0 #Initiate variable
lngth = length(topTermsByMonth)
y = topTermsByMonth[,2]  
months = topTermsByMonth[,1]
terms = colnames(topTermsByMonth)
figureOutput = "/" 
#i = 7
  
for(i in 3:lngth){
x = topTermsByMonth[,i]  
outliers = try(outlierTest(lm(y~x),cutoff=0.05,n.max=Inf))
outlierIndices = as.numeric(attr(outliers[[1]],"names"))
outlierRanks = c(x[outlierIndices],y[outlierIndices])
outlierTbl = data.frame(cbind(paste(months[outlierIndices]), terms[i],x[outlierIndices],y[outlierIndices]))
colnames(outlierTbl) <- c("Month", "Term", "Term Frequency", "Total Month's Entries")

if(length(outlierIndices) > 0
  && !is.na(outliers$bonf.p[[1]])
  && outliers$bonf.p[[1]] < sigCutoff){
                              
cat("\\begin{frame}[shrink=5]\n")
cat("\\frametitle{Term: ",terms[i],"}\n",sep="")

#Make LaTeX Table
LaTeXtable = xtable(format(outlierTbl,big.mark=",",scientific=FALSE,digits=2),label=paste(terms[i],sep=""),align="rllrr")
print(LaTeXtable,table.placement="tbp",size="\\tiny",big.mark=",",include.rownames=FALSE)

#Make outlier scatterplot
pdf(paste(terms[i],".pdf",sep=""))
try(scatterplot(y,x,xlab="Total Count of Entries",ylab="Term Frequency",id.n=5,labels=paste(months),boxplots=FALSE,smooth=FALSE,spread=FALSE),silent=FALSE)
dev.off()

cat("\\begin{figure}\n")
cat("\\centering\n")
cat("\\includegraphics[width=.60\\textwidth]{",terms[i],".pdf}\n",sep="")
cat("\\caption{Comparison of term frequency, ",terms[i],", with number of log entries}\n",sep="")
cat("\\label{fig:",terms[i],"}\n",sep="")
cat("\\end{figure}\n")
cat("\\end{frame}\n\n")

allOutliersTbl = rbind(temp, outlierTbl)
temp = allOutliersTbl
} #End if
} #End for loop
@ 

\begin{frame}[containsverbatim] 
\frametitle{Most Significant Outlier Terms and Months}    
\begin{itemize}
\item Can write a loop to summarize the most significant outlier terms in a table
\item Same terms as shown in preceding scatterplot graphs
\end{itemize}  
<<name="dictionary",echo=FALSE,quiet=TRUE,results=tex>>=  
xtable(allOutliersTbl[-1,]) #Negative index removes blank first row
@ 
\end{frame}

\end{document}
